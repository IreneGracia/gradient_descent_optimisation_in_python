# gradient_descent_optimisation_in_python

The notebooks in this repository aim to demonstrate and implement optimisation through the gradient descent in python. 

The gradient_descent_part I notebook intends to demonstrate parameter optimisation through the gradient descent procedure, by breaking into functions the steps of a single gradient descent epoch, and combining the individual functions in a loop to go through the entire optimisation procedure.

The  gradient_descent_part II notebook takes the exercise further and implements a vectorized gradient descent to minimize MSE loss, introduces early stopping and patience to prevent overfitting and drives efficiencies by looping over the dataset in mini-batches, and for each mini-batch updates beta.
